{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest: Classification by Automation Condition (By Participant Sampling)\n",
    "- Prediction of automation usage using all features and the top 20\n",
    "- We train and test by automation condition\n",
    "    - Grouping the data from all participants in a condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob as glob\n",
    "import os as os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose if running on Agave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGAVE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Choose automation condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "choose_condition  = 0\n",
    "conditions = ['SH','SL','FH','FL','ALL']\n",
    "cond = conditions[choose_condition]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to all data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files\n",
    "if(AGAVE==True):\n",
    "    files_path = '../../../NewFeatures/' + cond             # Agave\n",
    "else:\n",
    "    files_path = '../../../features_data_risk/' + cond      # Local\n",
    "\n",
    "all_files = glob.glob(os.path.join(files_path, \"*.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features that offer extra information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features that offer extra information (they're basically the same as boolAuto)\n",
    "features_extra = ['boolHand','boolTake','brakeOshp','sumAuto','sumHand','sumTake','sumTogg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features that are related to the operator's actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions\n",
    "features_internal = ['accAngOshpX','accAngOshpY','accAngOshpZ','accLinOshpX',\n",
    "\t\t\t\t\t'accLinOshpY','accLinOshpZ','boolButnA','boolButnB',\n",
    "\t\t\t\t\t'boolViolButn','boolViolLane','boolViolLead','boolViolPeds',\n",
    "\t\t\t\t\t'boolViolRang','boolViolTraf','boolViolVehs',\n",
    "\t\t\t\t\t'oriOshpX','oriOshpY','oriOshpZ','psnOshpLane','psnOshpLaneAbs',\n",
    "\t\t\t\t\t'psnOshpLaneLft','psnOshpX','psnOshpY','psnOshpZ',\n",
    "\t\t\t\t\t'steerOshp','sumViolButn','sumViolLane','sumViolLead','sumViolPeds',\n",
    "\t\t\t\t\t'sumViolRang','sumViolTraf','sumViolVehs','throtOshp','timeReact',\n",
    "\t\t\t\t\t'velAngOshpX','velAngOshpY','velAngOshpZ','velLinOshp','velLinOshpLane',\n",
    "\t\t\t\t\t'velLinOshpLaneAbs','velLinOshpLaneLft','velLinOshpRang','velLinOshpX',\n",
    "\t\t\t\t\t'velLinOshpY','velLinOshpZ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process data\n",
    "# Assign features from current time-step to future use of automation,\n",
    "# The idea is to predict future use of automation based on current behavior\n",
    "delay = 1/60            # How much time in advance we want to predict automation usage (in seconds)\n",
    "h = (int)(delay*60 - 1) # Turn the time into index (Considering a sampling rate of 60 Hz)\n",
    "data_list = []\n",
    "for file in all_files:\n",
    "    data_participant = pd.read_csv(file)\n",
    "    data_participant.insert(0,\"time\",[(i*1/60) for i in range(0,data_participant.shape[0])],True)\n",
    "    data_auto = data_participant['boolAuto']\n",
    "    data_auto = np.array(data_auto.iloc[h:])\n",
    "    data_participant = data_participant.drop('boolAuto', axis=1)\n",
    "    data_participant = data_participant.head(data_participant.shape[0]-h)\n",
    "    data_participant.insert(data_participant.shape[1],\"boolAuto\",data_auto,True)\n",
    "    data_list.append(data_participant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "SH_feats = ['psnOshpRang'    ,\n",
    "'score'          ,\n",
    "'velLinOshpX'    ,\n",
    "'velLinOshp'     ,\n",
    "'rrisk'          ,\n",
    "'sumViolLane'    ,\n",
    "'velLinLead'     ,\n",
    "'throtOshp'      ,\n",
    "'sumViolRang'    ,\n",
    "'odomRoad'       ,\n",
    "'velLinOshpRang' ,\n",
    "'sumViolPeds'    ,\n",
    "'ttcLead'        ,\n",
    "'sumViolButn'    ,\n",
    "'accLinOshpX'    ,\n",
    "'psnTrafPrxY'    ,\n",
    "'psnTrafPrxX'    ,\n",
    "'boolStatRang'   ,\n",
    "'oriTrafPrxZ'    ,\n",
    "'oriOshpY'       ]\n",
    "\n",
    "SL_feats = ['psnOshpRang'    ,\n",
    "'rrisk'          ,\n",
    "'sumViolAwrd'    ,\n",
    "'velLinOshp'     ,\n",
    "'throtOshp'      ,\n",
    "'velLinOshpX'    ,\n",
    "'sumViolRang'    ,\n",
    "'score'          ,\n",
    "'accLinOshpX'    ,\n",
    "'sumViolButn'    ,\n",
    "'oriOshpY'       ,\n",
    "'velLinLead'     ,\n",
    "'odomRoad'       ,\n",
    "'sumViolLane'    ,\n",
    "'velLinOshpRang' ,\n",
    "'psnTrafPrxX'    ,\n",
    "'psnTrafPrxY'    ,\n",
    "'accAngOshpY'    ,\n",
    "'oriLeadZ'       ,\n",
    "'velAngOshpY'    ]\n",
    "\n",
    "FH_feats = ['velLinOshpX'    ,\n",
    "'psnOshpRang'    ,\n",
    "'score'          ,\n",
    "'rrisk'          ,\n",
    "'velLinOshp'     ,\n",
    "'throtOshp'      ,\n",
    "'sumViolRang'    ,\n",
    "'velLinLead'     ,\n",
    "'odomRoad'       ,\n",
    "'sumViolButn'    ,\n",
    "'sumViolLane'    ,\n",
    "'accLinOshpX'    ,\n",
    "'velLinOshpRang' ,\n",
    "'psnOshpLaneLft' ,\n",
    "'oriTrafPrxZ'    ,\n",
    "'velAngLeadZAbs' ,\n",
    "'psnTrafPrxX'    ,\n",
    "'oriOshpY'       ,\n",
    "'psnTrafPrxY'    ,\n",
    "'oriOshpZ'       ]\n",
    "\n",
    "FL_feats = ['psnOshpRang' ,\n",
    "'sumViolAwrd' ,\n",
    "'velLinOshpX' ,\n",
    "'rrisk'       ,\n",
    "'throtOshp'   ,\n",
    "'velLinOshp'  ,\n",
    "'sumViolLane' ,\n",
    "'score'       ,\n",
    "'accLinOshpX' ,\n",
    "'odomRoad'    ,\n",
    "'sumViolRang' ,\n",
    "'oriTrafPrxZ' ,\n",
    "'sumViolButn' ,\n",
    "'psnTrafPrxX' ,\n",
    "'velLinLead'  ,\n",
    "'sumViolPeds' ,\n",
    "'psnLeadY'    ,\n",
    "'timeReact'   ,\n",
    "'psnTrafPrxY' ,\n",
    "'psnRoadY'    ]\n",
    "\n",
    "top_features_all = [SH_feats,SL_feats,FH_feats,FL_feats]\n",
    "top_feat_cond = top_features_all[choose_condition]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the performance results\n",
    "data_scores = {'Train_Acc':[],'Train_BalAcc':[],'Train_Prec':[],'Train_Rec':[],'Train_Spec':[],\n",
    "                'Train_AUC':[],'Test_Acc':[],'Test_BalAcc':[],'Test_Prec':[],'Test_Rec':[],\n",
    "                'Test_Spec':[],'Test_AUC':[]}\n",
    "perf_scores = pd.DataFrame(data=data_scores)\n",
    "# Save the ROC curve data\n",
    "data_roc = pd.DataFrame({'mean_fpr':[], 'tpr_1':[], 'tpr_2':[], 'tpr_3':[], 'tpr_4':[], \n",
    "                        'tpr_5':[]}) # 5 KFold\n",
    "# Save the best set of parameter for the model\n",
    "best_parameters = pd.DataFrame({'n_estimators':[], 'max_depth':[], 'max_features':[]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1204\\2550562612.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Get the best set of parameters, using Grid Search\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Number of trees in random forest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mn_estimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# Maximum number of levels in tree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmax_num_depth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m6000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the best set of parameters, using Grid Search\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 1000, num = 10)]\n",
    "# Maximum number of levels in tree\n",
    "max_num_depth = 6000\n",
    "if(AGAVE==True):\n",
    "    max_num_depth = max_num_depth/10\n",
    "max_depth = [int(x) for x in np.linspace(100, max_num_depth, num = 15)]\n",
    "max_depth.append(None)\n",
    "# Maximum number of features\n",
    "max_features = [4,5,8,10,15]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_depth': max_depth,\n",
    "               'max_features': max_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations (Number of paramater combinations to try)\n",
    "n_iter = 40\n",
    "# Indices of participants\n",
    "idx_part =list(range(0,16))\n",
    "idx_label = [0]*8 + [1]*8\n",
    "\n",
    "# Save the best parameters found\n",
    "SEED = 10\n",
    "best_params = {'n_estimators': 0,\n",
    "               'max_depth': 0,\n",
    "               'max_features': 0,\n",
    "               'auc': 0.0}\n",
    "\n",
    "for i in range(0, n_iter):\n",
    "    # Create the k-fold\n",
    "    cv = StratifiedKFold(n_splits=5)\n",
    "    # Choose a random combination of parameters\n",
    "    n_estim_rs = random.choice(random_grid['n_estimators'])\n",
    "    max_depth_rs = random.choice(random_grid['max_depth'])\n",
    "    max_feat_rs = random.choice(random_grid['max_features'])\n",
    "    # Create the classifier with the chosen parameters\n",
    "    classifier = RandomForestClassifier(n_estimators=n_estim_rs, max_depth=max_depth_rs,\n",
    "                                            max_features=max_feat_rs, n_jobs=-1)\n",
    "    # Scoring metric\n",
    "    auc_vals = []    \n",
    "    # Apply cross-validation\n",
    "    for _, (train, test) in enumerate(cv.split(idx_part,idx_label)):\n",
    "        train_list = [data_list[index] for index in train]\n",
    "        test_list = [data_list[index] for index in test]\n",
    "        # Read the data from the participants to be used for training and testing\n",
    "        df_data_train = pd.concat(train_list, ignore_index=True)\n",
    "        df_data_test = pd.concat(test_list, ignore_index=True)\n",
    "        # Delete the features that offer extra information from the dataset\n",
    "        df_data_train = df_data_train.drop(features_extra,axis = 1)\n",
    "        df_data_train = df_data_train.drop('time',axis=1)          # Drop the inserted time too\n",
    "        df_data_test = df_data_test.drop(features_extra,axis = 1)\n",
    "        df_data_test = df_data_test.drop('time',axis=1)\n",
    "        \n",
    "        # Check if running on Agave\n",
    "        if(AGAVE==True):\n",
    "            sample_train = df_data_train.sample(frac=0.15, random_state=SEED);\n",
    "            sample_test = df_data_test.sample(frac=0.15, random_state=SEED);\n",
    "        else:\n",
    "            sample_train = df_data_train.sample(frac=0.1, random_state=SEED);\n",
    "            sample_test = df_data_test.sample(frac=0.1, random_state=SEED);\n",
    "        \n",
    "        # Scale the data to a range [0,1]\n",
    "        idx_train = np.shape(sample_train)[0]\n",
    "        sample_comb = MinMaxScaler(copy=False).fit_transform(np.concatenate((sample_train[top_feat_cond].to_numpy(), \n",
    "                                        sample_test[top_feat_cond].to_numpy()), axis=0))\n",
    "        # Split scaled data into training and testing\n",
    "        X_train = sample_comb[0:idx_train]\n",
    "        y_train = sample_train['boolAuto']\n",
    "        X_test = sample_comb[idx_train:len(sample_comb)]\n",
    "        y_test = sample_test['boolAuto']\n",
    "\n",
    "        # Fit the model with the given training set\n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_pred_rs = classifier.predict(X_test)\n",
    "\n",
    "        # Compute Area under the ROC curve (the closer to 1, the better the estimator is)\n",
    "        auc_vals.append(roc_auc_score(y_test, y_pred_rs))\n",
    "\n",
    "    if(np.mean(auc_vals) > best_params['auc']):\n",
    "        best_params['auc'] = np.mean(auc_vals)\n",
    "        best_params['n_estimators'] = n_estim_rs\n",
    "        best_params['max_depth'] = max_depth_rs\n",
    "        best_params['max_features'] = max_feat_rs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best parameters fro random grid search\n",
    "best_n_estim = best_params['n_estimators']\n",
    "best_max_depth = best_params['max_depth']\n",
    "best_max_features = best_params['max_features']\n",
    "\n",
    "# Indices of participants\n",
    "idx_part = list(range(0, 16))\n",
    "idx_label = [0]*8 + [1]*8\n",
    "\n",
    "# Create the k-fold\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "# Create the classifier\n",
    "classifier = RandomForestClassifier(n_estimators=best_n_estim, max_depth=best_max_depth,\n",
    "                            max_features=best_max_features, n_jobs=-1)\n",
    "# Save performance metrics\n",
    "tprs = []; aucs = []; acc = []; acc_bal = []; prec = []; rec = []; spec = []; AUC_v = []\n",
    "acc_tr = []; acc_bal_tr = []; prec_tr = []; rec_tr = []; spec_tr = []; AUC_v_tr = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "# Apply cross-validation\n",
    "for _, (train, test) in enumerate(cv.split(idx_part,idx_label)):\n",
    "    train_list = [data_list[index] for index in train]\n",
    "    test_list = [data_list[index] for index in test]\n",
    "    # Read the data from the participants to be used for training and testing\n",
    "    df_data_train = pd.concat(train_list, ignore_index=True)\n",
    "    df_data_test = pd.concat(test_list, ignore_index=True)\n",
    "    # Delete the features that offer extra information from the dataset\n",
    "    df_data_train = df_data_train.drop(features_extra,axis = 1)\n",
    "    df_data_train = df_data_train.drop('time',axis=1)          # Drop the inserted time too\n",
    "    df_data_test = df_data_test.drop(features_extra,axis = 1)\n",
    "    df_data_test = df_data_test.drop('time',axis=1)\n",
    "    \n",
    "    # Check if running on Agave\n",
    "    if(AGAVE==True):\n",
    "        sample_train = df_data_train.sample(frac=1, random_state=SEED);\n",
    "        sample_test = df_data_test.sample(frac=1, random_state=SEED);\n",
    "    else:\n",
    "        sample_train = df_data_train.sample(frac=0.1, random_state=SEED);\n",
    "        sample_test = df_data_test.sample(frac=0.1, random_state=SEED);\n",
    "    \n",
    "    # Scale the data to a range [0,1]\n",
    "    idx_train = np.shape(sample_train)[0]\n",
    "    sample_comb = MinMaxScaler(copy=False).fit_transform(np.concatenate((sample_train[top_feat_cond].to_numpy(), \n",
    "                                    sample_test[top_feat_cond].to_numpy()), axis=0))\n",
    "    # Split scaled data into training and testing\n",
    "    X_train = sample_comb[0:idx_train]\n",
    "    y_train = sample_train['boolAuto']\n",
    "    X_test = sample_comb[idx_train:len(sample_comb)]\n",
    "    y_test = sample_test['boolAuto']\n",
    "\n",
    "    # Fit the model with the given training set\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Get predictions for the training and testing sets\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    y_pred_tr = classifier.predict(X_train)\n",
    "\n",
    "    # ROC curve metrics\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred, pos_label=1)\n",
    "    interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "\n",
    "    # Other performance metrics\n",
    "    # Testing\n",
    "    acc.append(accuracy_score(y_test,y_pred))\n",
    "    acc_bal.append(balanced_accuracy_score(y_test,y_pred))\n",
    "    prec.append(precision_score(y_test,y_pred))\n",
    "    rec.append(recall_score(y_test,y_pred, pos_label=1))\n",
    "    spec.append(recall_score(y_test,y_pred, pos_label=0))\n",
    "    AUC_v.append(roc_auc_score(y_test,y_pred))\n",
    "    # Training\n",
    "    acc_tr.append(accuracy_score(y_train,y_pred_tr))\n",
    "    acc_bal_tr.append(balanced_accuracy_score(y_train,y_pred_tr))\n",
    "    prec_tr.append(precision_score(y_train,y_pred_tr))\n",
    "    rec_tr.append(recall_score(y_train,y_pred_tr, pos_label=1))\n",
    "    spec_tr.append(recall_score(y_train,y_pred_tr, pos_label=0))\n",
    "    AUC_v_tr.append(roc_auc_score(y_train,y_pred_tr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the dataframe with all the results\n",
    "perf_scores.Train_Acc = acc_tr\n",
    "perf_scores.Train_BalAcc = acc_bal_tr\n",
    "perf_scores.Train_Prec = prec_tr\n",
    "perf_scores.Train_Rec = rec_tr\n",
    "perf_scores.Train_Spec = spec_tr\n",
    "perf_scores.Train_AUC = AUC_v_tr\n",
    "perf_scores.Test_Acc = acc\n",
    "perf_scores.Test_BalAcc\t = acc_bal\n",
    "perf_scores.Test_Prec = prec\n",
    "perf_scores.Test_Rec = rec\n",
    "perf_scores.Test_Spec = spec\n",
    "perf_scores.Test_AUC = AUC_v\n",
    "# Best parameters\n",
    "best_parameters.n_estimators = [best_n_estim]\n",
    "best_parameters.max_depth = [best_max_depth]\n",
    "best_parameters.max_features = [best_max_features]\n",
    "# Data ROC curves\n",
    "data_roc.mean_fpr = mean_fpr\n",
    "for i in range(1,6):\n",
    "    data_roc.iloc[:,i] = tprs[i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to .csv files\n",
    "folder = 'Results_CM_ByPar'\n",
    "perf_scores.to_csv(folder + '/RF_CBPS_performance_' + cond + '.csv', index=False)\n",
    "data_roc.to_csv(folder + '/RF_CBPS_ROC_' + cond + '.csv', index=False)\n",
    "best_parameters.to_csv(folder + '/RF_CBPS_BestParm_' + cond + '.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3 (v3.7.3:ef4ec6ed12, Mar 25 2019, 22:22:05) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b941f43c886fd79c9cadd6d79d1202cd65c517310108e5ac245f0d9f6cd0a3e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
