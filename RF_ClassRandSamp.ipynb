{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest: Classification by Automation Condition (Random Sampling)\n",
    "- Prediction of automation usage using all features and the top 20\n",
    "- We train and test by automation condition\n",
    "    - Grouping the data from all participants in a condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob as glob\n",
    "import os as os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose if running on Agave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGAVE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Choose automation condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "choose_condition  = 0\n",
    "conditions = ['SH','SL','FH','FL','ALL']\n",
    "cond = conditions[choose_condition]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to all data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files\n",
    "if(AGAVE==True):\n",
    "    files_path = '../../../NewFeatures/' + cond             # Agave\n",
    "else:\n",
    "    files_path = '../../../features_data_risk/' + cond      # Local\n",
    "\n",
    "all_files = glob.glob(os.path.join(files_path, \"*.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features that offer extra information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features that offer extra information (they're basically the same as boolAuto)\n",
    "features_extra = ['boolHand','boolTake','brakeOshp','sumAuto','sumHand','sumTake','sumTogg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features that are related to the operator's actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions\n",
    "features_internal = ['accAngOshpX','accAngOshpY','accAngOshpZ','accLinOshpX',\n",
    "\t\t\t\t\t'accLinOshpY','accLinOshpZ','boolButnA','boolButnB',\n",
    "\t\t\t\t\t'boolViolButn','boolViolLane','boolViolLead','boolViolPeds',\n",
    "\t\t\t\t\t'boolViolRang','boolViolTraf','boolViolVehs',\n",
    "\t\t\t\t\t'oriOshpX','oriOshpY','oriOshpZ','psnOshpLane','psnOshpLaneAbs',\n",
    "\t\t\t\t\t'psnOshpLaneLft','psnOshpX','psnOshpY','psnOshpZ',\n",
    "\t\t\t\t\t'steerOshp','sumViolButn','sumViolLane','sumViolLead','sumViolPeds',\n",
    "\t\t\t\t\t'sumViolRang','sumViolTraf','sumViolVehs','throtOshp','timeReact',\n",
    "\t\t\t\t\t'velAngOshpX','velAngOshpY','velAngOshpZ','velLinOshp','velLinOshpLane',\n",
    "\t\t\t\t\t'velLinOshpLaneAbs','velLinOshpLaneLft','velLinOshpRang','velLinOshpX',\n",
    "\t\t\t\t\t'velLinOshpY','velLinOshpZ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process data\n",
    "# Assign features from current time-step to future use of automation,\n",
    "# The idea is to predict future use of automation based on current behavior\n",
    "delay = 1/60            # How much time in advance we want to predict automation usage (in seconds)\n",
    "h = (int)(delay*60 - 1) # Turn the time into index (Considering a sampling rate of 60 Hz)\n",
    "data_list = []\n",
    "for file in all_files:\n",
    "    data_participant = pd.read_csv(file)\n",
    "    data_participant.insert(0,\"time\",[(i*1/60) for i in range(0,data_participant.shape[0])],True)\n",
    "    data_auto = data_participant['boolAuto']\n",
    "    data_auto = np.array(data_auto.iloc[h:])\n",
    "    data_participant = data_participant.drop('boolAuto', axis=1)\n",
    "    data_participant = data_participant.head(data_participant.shape[0]-h)\n",
    "    data_participant.insert(data_participant.shape[1],\"boolAuto\",data_auto,True)\n",
    "    data_list.append(data_participant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the data from all participants together\n",
    "df_data = pd.concat(data_list, ignore_index=True) # Read the data from all participants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete features that offer extra information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the features that offer extra information from the dataset\n",
    "# The idea is to determine which features are more related to automation usage\n",
    "df_data = df_data.drop(features_extra,axis = 1)\n",
    "df_data = df_data.drop('time',axis=1)               # Drop the inserted time too\n",
    "# df_data = df_data.drop(features_internal,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take just a sample of the data for speed\n",
    "# Change this when running on Agave\n",
    "SEED = 10\n",
    "if(AGAVE==True):\n",
    "    sample_data = df_data.sample(frac=1, random_state=SEED);\n",
    "else:\n",
    "    sample_data = df_data.sample(frac=0.1, random_state=SEED);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = sample_data.drop('boolAuto',axis = 1)\n",
    "y_data = sample_data['boolAuto']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SH_feats = ['psnOshpRang'    ,\n",
    "'score'          ,\n",
    "'velLinOshpX'    ,\n",
    "'velLinOshp'     ,\n",
    "'rrisk'          ,\n",
    "'sumViolLane'    ,\n",
    "'velLinLead'     ,\n",
    "'throtOshp'      ,\n",
    "'sumViolRang'    ,\n",
    "'odomRoad'       ,\n",
    "'velLinOshpRang' ,\n",
    "'sumViolPeds'    ,\n",
    "'ttcLead'        ,\n",
    "'sumViolButn'    ,\n",
    "'accLinOshpX'    ,\n",
    "'psnTrafPrxY'    ,\n",
    "'psnTrafPrxX'    ,\n",
    "'boolStatRang'   ,\n",
    "'oriTrafPrxZ'    ,\n",
    "'oriOshpY'       ]\n",
    "\n",
    "SL_feats = ['psnOshpRang'    ,\n",
    "'rrisk'          ,\n",
    "'sumViolAwrd'    ,\n",
    "'velLinOshp'     ,\n",
    "'throtOshp'      ,\n",
    "'velLinOshpX'    ,\n",
    "'sumViolRang'    ,\n",
    "'score'          ,\n",
    "'accLinOshpX'    ,\n",
    "'sumViolButn'    ,\n",
    "'oriOshpY'       ,\n",
    "'velLinLead'     ,\n",
    "'odomRoad'       ,\n",
    "'sumViolLane'    ,\n",
    "'velLinOshpRang' ,\n",
    "'psnTrafPrxX'    ,\n",
    "'psnTrafPrxY'    ,\n",
    "'accAngOshpY'    ,\n",
    "'oriLeadZ'       ,\n",
    "'velAngOshpY'    ]\n",
    "\n",
    "FH_feats = ['velLinOshpX'    ,\n",
    "'psnOshpRang'    ,\n",
    "'score'          ,\n",
    "'rrisk'          ,\n",
    "'velLinOshp'     ,\n",
    "'throtOshp'      ,\n",
    "'sumViolRang'    ,\n",
    "'velLinLead'     ,\n",
    "'odomRoad'       ,\n",
    "'sumViolButn'    ,\n",
    "'sumViolLane'    ,\n",
    "'accLinOshpX'    ,\n",
    "'velLinOshpRang' ,\n",
    "'psnOshpLaneLft' ,\n",
    "'oriTrafPrxZ'    ,\n",
    "'velAngLeadZAbs' ,\n",
    "'psnTrafPrxX'    ,\n",
    "'oriOshpY'       ,\n",
    "'psnTrafPrxY'    ,\n",
    "'oriOshpZ'       ]\n",
    "\n",
    "FL_feats = ['psnOshpRang' ,\n",
    "'sumViolAwrd' ,\n",
    "'velLinOshpX' ,\n",
    "'rrisk'       ,\n",
    "'throtOshp'   ,\n",
    "'velLinOshp'  ,\n",
    "'sumViolLane' ,\n",
    "'score'       ,\n",
    "'accLinOshpX' ,\n",
    "'odomRoad'    ,\n",
    "'sumViolRang' ,\n",
    "'oriTrafPrxZ' ,\n",
    "'sumViolButn' ,\n",
    "'psnTrafPrxX' ,\n",
    "'velLinLead'  ,\n",
    "'sumViolPeds' ,\n",
    "'psnLeadY'    ,\n",
    "'timeReact'   ,\n",
    "'psnTrafPrxY' ,\n",
    "'psnRoadY'    ]\n",
    "\n",
    "top_features_all = [SH_feats,SL_feats,FH_feats,FL_feats]\n",
    "top_feat_cond = top_features_all[choose_condition]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the dataset to consider only the top num_feats features\n",
    "X_data_cut = X_data[top_feat_cond]\n",
    "# Scale the data (0,1)\n",
    "X_data_cut = MinMaxScaler().fit_transform(X_data_cut)\n",
    "y_data = np.array(y_data.to_list())\n",
    "# X_data = X_data[top_feat_cond]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing\n",
    "frac = 0.8 # Choose fraction of data to use for training\n",
    "n_samples = len(X_data_cut)\n",
    "idx_split = (int)(np.round(frac*n_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features and target\n",
    "train_labels = np.array(y_data[0:idx_split])\n",
    "test_labels = np.array(y_data[idx_split:n_samples]) \n",
    "train_features = np.array(X_data_cut[0:idx_split,:])\n",
    "test_features = np.array(X_data_cut[idx_split:n_samples,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (55765, 20)\n",
      "Testing Data Shape: (13941, 20)\n"
     ]
    }
   ],
   "source": [
    "# Check sizes\n",
    "print('Training Data Shape:', train_features.shape)\n",
    "print('Testing Data Shape:', test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the performance results\n",
    "data_scores = {'Train_Acc':[],'Train_BalAcc':[],'Train_Prec':[],'Train_Rec':[],'Train_Spec':[],\n",
    "                'Train_AUC':[],'Test_Acc':[],'Test_BalAcc':[],'Test_Prec':[],'Test_Rec':[],\n",
    "                'Test_Spec':[],'Test_AUC':[]}\n",
    "perf_scores = pd.DataFrame(data=data_scores)\n",
    "# Save the ROC curve data\n",
    "data_roc = pd.DataFrame({'mean_fpr':[], 'tpr_1':[], 'tpr_2':[], 'tpr_3':[], 'tpr_4':[], \n",
    "                        'tpr_5':[]}) # 5 KFold\n",
    "# Save the best set of parameter for the model\n",
    "best_parameters = pd.DataFrame({'n_estimators':[], 'max_depth':[], 'max_features':[]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best set of parameters, using Grid Search\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 1000, num = 10)]\n",
    "# Maximum number of levels in tree\n",
    "max_num_depth = 6000\n",
    "if(AGAVE==True):\n",
    "    max_num_depth = max_num_depth*10\n",
    "max_depth = [int(x) for x in np.linspace(100, max_num_depth, num = 15)]\n",
    "max_depth.append(None)\n",
    "# Maximum number of features\n",
    "max_features = [4,5,8,10,15]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_depth': max_depth,\n",
    "               'max_features': max_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100,\n",
       " 521,\n",
       " 942,\n",
       " 1364,\n",
       " 1785,\n",
       " 2207,\n",
       " 2628,\n",
       " 3050,\n",
       " 3471,\n",
       " 3892,\n",
       " 4314,\n",
       " 4735,\n",
       " 5157,\n",
       " 5578,\n",
       " 6000,\n",
       " None]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(), n_jobs=6,\n",
       "                   param_distributions={'max_depth': [10, 1204, 2398, 3592,\n",
       "                                                      4787, 5981, 7175, 8369,\n",
       "                                                      9564, 10758, 11952, 13146,\n",
       "                                                      14341, 15535, 16729,\n",
       "                                                      None],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [10, 120, 230, 340, 450,\n",
       "                                                         560, 670, 780, 890,\n",
       "                                                         1000]},\n",
       "                   random_state=10, verbose=2)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf_gs = RandomForestClassifier(n_jobs=-1)\n",
    "# Random search of parameters, using 5 fold cross validation, \n",
    "# Search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf_gs, param_distributions = random_grid, \n",
    "                                n_iter = 100, cv = 5, verbose=2, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation (5-fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation when the best parameters have been selected\n",
    "best_n_estim = rf_random.best_params_['n_estimators']\n",
    "best_max_depth = rf_random.best_params_['max_depth']\n",
    "best_max_features = rf_random.best_params_['max_features']\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "classifier = RandomForestClassifier(n_estimators=best_n_estim, max_depth=best_max_depth,\n",
    "                            max_features=best_max_features, n_jobs=-1)\n",
    "\n",
    "tprs = []; aucs = []; acc = []; acc_bal = []; prec = []; rec = []; spec = []; AUC_v = []\n",
    "acc_tr = []; acc_bal_tr = []; prec_tr = []; rec_tr = []; spec_tr = []; AUC_v_tr = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "for i, (train, test) in enumerate(cv.split(X_data_cut, y_data)):\n",
    "    # Classifier training and testing\n",
    "    y_train = y_data[train]\n",
    "    classifier.fit(X_data_cut[train], y_train)\n",
    "    y_test = y_data[test]\n",
    "    y_pred = classifier.predict(X_data_cut[test])\n",
    "    y_pred_tr = classifier.predict(X_data_cut[train])\n",
    "\n",
    "    # ROC curve metrics\n",
    "    fpr, tpr, _ = roc_curve(y_data[test], y_pred, pos_label=1)\n",
    "    interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "\n",
    "    # Other performance metrics\n",
    "    # Testing\n",
    "    acc.append(accuracy_score(y_test,y_pred))\n",
    "    acc_bal.append(balanced_accuracy_score(y_test,y_pred))\n",
    "    prec.append(precision_score(y_test,y_pred))\n",
    "    rec.append(recall_score(y_test,y_pred, pos_label=1))\n",
    "    spec.append(recall_score(y_test,y_pred, pos_label=0))\n",
    "    AUC_v.append(roc_auc_score(y_test,y_pred))\n",
    "    # Training\n",
    "    acc_tr.append(accuracy_score(y_train,y_pred_tr))\n",
    "    acc_bal_tr.append(balanced_accuracy_score(y_train,y_pred_tr))\n",
    "    prec_tr.append(precision_score(y_train,y_pred_tr))\n",
    "    rec_tr.append(recall_score(y_train,y_pred_tr, pos_label=1))\n",
    "    spec_tr.append(recall_score(y_train,y_pred_tr, pos_label=0))\n",
    "    AUC_v_tr.append(roc_auc_score(y_train,y_pred_tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the dataframe with all the results\n",
    "perf_scores.Train_Acc = acc_tr\n",
    "perf_scores.Train_BalAcc = acc_bal_tr\n",
    "perf_scores.Train_Prec = prec_tr\n",
    "perf_scores.Train_Rec = rec_tr\n",
    "perf_scores.Train_Spec = spec_tr\n",
    "perf_scores.Train_AUC = AUC_v_tr\n",
    "perf_scores.Test_Acc = acc\n",
    "perf_scores.Test_BalAcc\t = acc_bal\n",
    "perf_scores.Test_Prec = prec\n",
    "perf_scores.Test_Rec = rec\n",
    "perf_scores.Test_Spec = spec\n",
    "perf_scores.Test_AUC = AUC_v\n",
    "# Best parameters\n",
    "best_parameters.n_estimators = [best_n_estim]\n",
    "best_parameters.max_depth = [best_max_depth]\n",
    "best_parameters.max_features = [best_max_features]\n",
    "# Data ROC curves\n",
    "data_roc.mean_fpr = mean_fpr\n",
    "for i in range(1,6):\n",
    "    data_roc.iloc[:,i] = tprs[i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to .csv files\n",
    "folder = 'Resuls_CM'\n",
    "perf_scores.to_csv(folder + '/RF_CRS_performance_' + cond + '.csv', index=False)\n",
    "data_roc.to_csv(folder + '/RF_CRS_ROC_' + cond + '.csv', index=False)\n",
    "best_parameters.to_csv(folder + '/RF_CRS_BestParm_' + cond + '.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b941f43c886fd79c9cadd6d79d1202cd65c517310108e5ac245f0d9f6cd0a3e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
